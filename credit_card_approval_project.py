# -*- coding: utf-8 -*-
"""Credit Card approval project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eQ3wpZPOB3OcJv88zpaEO0CMFjVFOcJB

# Credit Card approval project

## Introduction

A credit card is a physical payment card issued by a financial institution, typically a bank, that allows you to borrow money within a pre-approved credit limit. You can use this credit to purchase goods and services or withdraw cash from ATMs.

Here are some key things to know about credit cards:

**Functionality:**

1. Borrowing money: You are essentially borrowing money from the issuer up to your credit limit. This means you need to pay back the borrowed amount, usually within a monthly billing cycle.
2. Interest charges: If you don't pay back the balance in full by the due date, you will be charged interest on the remaining amount. The interest rate can vary depending on the card and your creditworthiness.
3. Repayment: You can typically make payments towards your credit card balance through online banking, mobile apps, or by mail.

**Benefits:**

1. Convenience: Credit cards offer a convenient way to pay for things without carrying cash. They are also widely accepted by merchants, both online and in-person.
2. Rewards: Many credit cards offer rewards programs that give you points, miles, or cash back for your purchases. These can be redeemed for various rewards, such as travel, merchandise, or gift cards.
3. Building credit: Using a credit card responsibly and paying your bills on time can help you build a good credit history. This can make it easier to qualify for loans, mortgages, and other forms of credit in the future.

**Drawbacks:**

1. Debt: If you are not careful, it's easy to overspend with a credit card and accumulate debt. This can lead to high interest charges and financial difficulties.
2. Annual fees: Some credit cards, especially those with generous rewards programs, may have annual fees.
3. Interest rates: Credit card interest rates can be high, so it's important to pay your balance in full every month to avoid paying interest.

## Importance of this project

Credit card approval projects can be important for several reasons, both for customers and for financial institutions. Here are some key aspects to consider:

1. For financial institutions:

   a. Reduced Risk: Accurate approval models enable lending to creditworthy customers, minimizing defaults and losses.

   b. Increased Profitability: Approving more eligible customers expands the customer base, leading to higher transaction volume and revenue.
   
   c. Improved Efficiency: Automated models streamline the application process, saving time and resources.

   d. Data-driven Decisions: Models provide insights into customer behavior and risk factors, facilitating informed credit decisions.

   e. Competitive Advantage: Advanced models offer institutions a competitive edge in attracting and retaining customers.


2. For customers:

   a. Faster Access to Credit: Accurate models offer quicker approval decisions, reducing waiting times and frustration.

   b. Fairer Lending: Models can mitigate bias and ensure credit access is based on objective criteria.

   c. Improved Financial Inclusion: Expanding access to credit can promote financial health and well-being.

   d. Personalized Offers: Models can help tailor credit card options to individual needs and financial goals.
  
   e. Increased Financial Security: Responsible credit card use can build credit history, providing access to better terms and rates in the future.

## Impact on Bank sector

Banks receive a lot of credit card applications. Many of the applications do not get approved for a variety of reasons, like increased loan balances or poor-income levels. Manually analysing these applications can be very time consuming and full of human errors. Hence we can automate this task with the help of machine learning.

# Objective

The objective of this project is to build a machine-learning model that accurately predicts credit card approval outcomes. The model can provide insights into the most important factors that impact credit card approval, allowing banks to make informed decisions quickly and accurately. This can result in a faster and more efficient credit card approval process, reducing the time and cost associated with manual credit assessment methods.

Overall, the proposed project has the potential to upgrade the credit card approval process, reduce manual effort and errors, and enhance the customer experience. This project demonstrates the potential of machine learning in automating and improving critical banking processes and can have significant benefits for banks and financial institutions in the global market.

## Steps for the project:

1. Import libraries and dataset

- Import libraries like pandas, numpy, sklearn, matplot, seaborn. Next we will import the two datasets in the form of csv files using pandas dataframes.
- Next we will merge the two dataframes into one dataframe for data preprocessing

2. Data Preprocessing
- We will perform data preprocessing techniques like handling missing values, removing duplicates, handling outliers, data scaling and normalization, encoding categorial data, feature scaling and handling imbalanced data.
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/home/Credit_card (2).csv')

df.head()

df=pd.merge(df,df,how='outer',on='Ind_ID') #how is the type of join mentioned, on is the common column that joins both the dataset
df

df.shape

df.info()

"""We can see that there are misssing values in 'GENDER','Annual_income','Birthday_count','Type_Occupation' columns.
- int - 9 columns- 'Ind_ID','CHILDREN','Employed_days','Mobile_phone','Work_Phone','Phone','EMAIL_ID','Family_Members','label'
- object - 8 columns- 'GENDER','Car_Owner','Propert_Owner','Type_Income','EDUCATION','Marital_status','Housing_type','Type_Occupation'
- float - 2 columns- 'Annual_income','Birthday_count'

## 2. Data Preprocessing

Check null values - isnull() is widely been in all pre-processing steps to identify null values in the data
"""

df.isnull().sum()

# calculate the percentage of missing values in each column
miss_percent = ((df.isnull().sum()/len(df))*100).round(decimals=2)
miss_percent

df['GENDER_x'].unique()

df[df['GENDER_x']=='M'].count()

df[df['GENDER_x']=='F'].count()

df['GENDER_x']=df['GENDER_x'].fillna('F')
df['GENDER_x'].unique()

df['GENDER_x'].value_counts()

mean_annual_inocme_x = df['Annual_income_x'].mean()
mean_annual_inocme_x

df['Annual_income_x']=df['Annual_income_x'].fillna(mean_annual_inocme_x)

df['Birthday_count_y']=df['Birthday_count_y'].fillna(df['Birthday_count_y'].mean())

df['Type_Occupation_y'].value_counts()

df['Type_Occupation_y']=df['Type_Occupation_y'].fillna('Laborers')

df.isnull().sum()

df.nunique()

df.duplicated().value_counts() # checks duplicate entries

# checking for duplicate values without considering the 'Ind_ID' column
duplicates = df.duplicated(subset=['GENDER_x','Car_Owner_y','Propert_Owner_y','CHILDREN_y','Annual_income_x','Type_Income_x','EDUCATION_x'
                                   ,'Marital_status_y','Housing_type_y','Birthday_count_y','Employed_days_y','Mobile_phone_y','Work_Phone_y'
                                   ,'Phone_y','EMAIL_ID_y','Type_Occupation_y','Family_Members_y'])
duplicates.value_counts()

df.shape

df = df.drop_duplicates(subset=['GENDER_x','Car_Owner_y','Propert_Owner_y','CHILDREN_y','Annual_income_x','Type_Income_x','EDUCATION_x'
                                   ,'Marital_status_y','Housing_type_y','Birthday_count_y','Employed_days_y','Mobile_phone_y','Work_Phone_y'
                                   ,'Phone_y','EMAIL_ID_y','Type_Occupation_y','Family_Members_y'])
df.shape

# Lets rename some of the columns accurately
df = df.rename(columns={"GENDER":"Gender","CHILDREN":"Children","EMAIL_ID":"Email_ID","Propert_Owner":"Property_Owner"
                        ,"EDUCATION":"Education","label":"Label","Annual_income":"Annual_Income"})
df.head(3)

df.shape

df.describe()

df.to_csv('/home/Final_dataset (1).csv')

df.columns

print(df.columns)

"""## 3. EDA (Univariate and Bivariate Analysis)

We shall now perform univariate and bivariate analysis. Univariate and bivariate analysis are fundamental techniques in data analysis.

1. Univariate Analysis:- Uni means one and variate means variable, so in univariate analysis, there is only one dependable variable.

   **Objectives:-**
   - Characterize its distribution: Central tendency (mean, median, mode), spread (variance, standard deviation, range), shape (skewness, kurtosis).
   - Identify outliers: Extreme values that deviate significantly from the rest of the data.
   - Detect potential errors or anomalies: Data quality issues or unusual patterns.
   - Gain insights into individual variables: Their characteristics and potential issues.

   **Methods:-**

   *Numerical variables:-*
   - Frequency distribution tables
   - Histogram
   - Box plot
   - Density plot

    *Categorial variables*
   - Pie chart
   - Count plot
   - Bar chart
   - Column chart

2. Bivariate Analysis:- Bi means two and variate means variable, so here there are two variables. The analysis is related to cause and the relationship between the two variables.

**Objectives:-**
   - Explore correlations: Strength and direction of linear relationships.
   - Identify associations: Potential patterns or connections between variables.
   - Generate hypotheses for further investigation: Ideas for more in-depth analysis.
   - Uncover potential explanatory factors: Variables that might influence others.

    **Methods:-**
    
    *Numerical variables*
    - Scatter plot
    - Linear Correlation
    - Pairplot
    
    *Categorial variables*
    - Chi-Square test
    - Z-test and T-test
    - Analysis of Variance (ANOVA)


"""

# Lets divide the variables into two groups (Categorial variables and Numerical variables)
cat_cols=df.select_dtypes(include=['object']).columns
num_cols=df.select_dtypes(include=['number']).columns
print('Categorial variables:-')
print(cat_cols)
print('Numerical variables:-')
print(num_cols)

"""### EDA (Univariate Analysis)

#### (a) Countplot
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Creating the subplot structure
fig, axes = plt.subplots(4, 2, figsize=(18, 18))
fig.suptitle('Bar plot for all categorical variables in the dataset', fontsize=16)

# Plotting each categorical variable
sns.countplot(ax=axes[0, 0], x='GENDER_x', data=df, color='blue', order=df['GENDER_x'].value_counts().index)
sns.countplot(ax=axes[0, 1], x='Car_Owner_x', data=df, color='green', order=df['Car_Owner_x'].value_counts().index)
sns.countplot(ax=axes[1, 0], x='Propert_Owner_x', data=df, color='yellow', order=df['Propert_Owner_x'].value_counts().index)
sns.countplot(ax=axes[1, 1], x='Type_Income_y', data=df, color='orange', order=df['Type_Income_y'].value_counts().index)
sns.countplot(ax=axes[2, 0], x='EDUCATION_x', data=df, color='red', order=df['EDUCATION_x'].value_counts().index)
sns.countplot(ax=axes[2, 1], x='Marital_status_y', data=df, color='purple', order=df['Marital_status_y'].value_counts().index)
sns.countplot(ax=axes[3, 0], x='Housing_type_y', data=df, color='black', order=df['Housing_type_y'].value_counts().index)
sns.countplot(ax=axes[3, 1], x='Type_Occupation_y', data=df, color='darkblue', order=df['Type_Occupation_y'].value_counts().index)

# Adjusting tick labels rotation for better readability
axes[2, 0].tick_params(labelrotation=10)
axes[3, 0].tick_params(labelrotation=90)
axes[3, 1].tick_params(labelrotation=90)

# Adjust layout and show the plot
plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust the rect to fit the suptitle
plt.show()

# Univariate Analysis (Numerical variables) using histplot
fig,axes = plt.subplots(6,2,figsize=(20,20))
sns.histplot(ax=axes[0,0],data=df['Ind_ID'],kde=True)
sns.histplot(ax=axes[0,1],data=df['CHILDREN_x'],kde=True)
sns.histplot(ax=axes[1,0],data=df['Annual_income_x'],kde=True)
sns.histplot(ax=axes[1,1],data=df['Birthday_count_x'],kde=True)
sns.histplot(ax=axes[2,0],data=df['Employed_days_x'],kde=True)
sns.histplot(ax=axes[2,1],data=df['Mobile_phone_x'],kde=True)
sns.histplot(ax=axes[3,0],data=df['Work_Phone_x'],kde=True)
sns.histplot(ax=axes[3,1],data=df['Phone_x'],kde=True)
sns.histplot(ax=axes[4,0],data=df['EMAIL_ID_x'],kde=True)
sns.histplot(ax=axes[4,1],data=df['Family_Members_x'],kde=True)

print("Skew of Ind_ID:",df['Ind_ID'].skew())
print("Skew of CHILDREN_x:",df['CHILDREN_x'].skew())
print("Skew of Annual_income_x:",df['Annual_income_x'].skew())
print("Skew of Employed_days_x:",df['Employed_days_x'].skew())
print("Skew of Birthday_count_x:",df['Birthday_count_x'].skew())
print("Skew of Mobile_phone_x:",df['Mobile_phone_x'].skew())
print("Skew of Work_Phone_x:",df['Work_Phone_x'].skew())
print("Skew of Phone_x:",df['Phone_x'].skew())
print("Skew of EMAIL_ID_x':",df['EMAIL_ID_x'].skew())
print("Skew of Family_Members_x:",df['Family_Members_x'].skew())
plt.show()

"""**Insights:-**

- Majority of the applicants are female which shows that the majority of the customers are women.
- Majority of the applicants do not own a car but own property.
- Majority of the applicants don't have children.
- From Annual Income histogram, the plot is right skewed or positive skewed. Here mean>median>mode are all positive values and not zero or negative. A large no. of data are placed on the left side.
- From the above histogram we can observe most of the applicants source of income is through working.
- Most of the applicants education level is secondary/secondary special.
- Most of the applicants are married.
- Most of the applicants own house/apartments.
- Birthday count values are normally distributed. The plot is almost zero skewed i.e; the plot is symmetrical.
- Many of the applicants have lesser employed days.
- Every applicant has a mobile phone.
- Majority of the applicants don't have a working phone.
- Majority of the applicants don't have Email-ID.
- Most of the applicants are working as labourers.
- Majority of the applicants only have two members in a family
- Most of the applicants were eligible for credit card, however there were applicants whose application were rejected.

### EDA (Bivariate Analysis)

When we talk about bivariate analysis, it means analyzing 2 variables. Since we know there are numerical and categorical variables, there is a way of analyzing these variables as shown below:

1. Numerical vs. Numerical
- Scatterplot
- Line plot
- Heatmap for correlation
- Joint plot

2. Categorical vs. Numerical
- Bar chart
- Violin plot
- Categorical box plot
- Swarm plot

3. Two Categorical Variables
- Bar chart
- Grouped bar chart
- Point plot
"""

# numerical vs numerical (scatter plot)
# scatter_var=['Label'] # Rest of the columns vs label willbe plotted
for column in df.columns:
    plt.figure()
    plt.scatter(df[column].astype(str), df['Ind_ID'].astype(str))
    plt.title(f'Scatter plot of {column} and Ind_ID ')
    plt.xlabel(f'{column}')
    plt.ylabel('Ind_ID')
    plt.show()

sns.pairplot(df)

df.info()

"""## 4. Feature Engineering

Feature engineering involves creating new features or transforming existing features to
enhance the performance of machine learning models. It aims to provide the model with
more relevant and informative input variables.

### (a) Handling Outliers

An Outlier is an observation in a given dataset that lies far from the rest of the observations. That means an outlier is vastly larger or smaller than the remaining values in the set.

Here in this scenario since the annual income is the most important factor, we will remove outliers in the annual income column
"""

mean = df['Annual_income_x'].mean().round(0)
std_dev = df['Annual_income_x'].std().round(0)
max = df['Annual_income_x'].max().round(0)
min = df['Annual_income_x'].min().round(0)

plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
sns.distplot(df['Annual_income_x'])
print("Skew:",df['Annual_income_x'].skew().round(2))
print("Mean:",mean)
print("Std_dev:",std_dev)
print("Max:",max)
print("Min:",min)

plt.subplot(1,2,2)
sns.boxplot(df['Annual_income_x']) # Changed column name to 'Annual_income_x'
plt.xlabel("Annual_income_x")
plt.ylabel("Values")
plt.title("Vertical Boxplot")
plt.show()

outlier = df[df['Annual_income_x']>1000000.0]
outlier

"""There is one outlier which has an annual income of 1575000 which is disturbing the overall ML model. We will eleminate this outlier."""

df = df.drop(df[df['Annual_income_x']> 1000000.0].index)
df.head(4)

df.shape

df.info()

plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
sns.distplot(df['Annual_income_x'])
print("Skew:",df['Annual_income_x'].skew().round(2))
print("Mean:",mean)
print("Std_dev:",std_dev)
print("Max:",max)
print("Min:",min)

plt.subplot(1,2,2)
sns.boxplot(df.Annual_income_x)
plt.xlabel("Annual_income_x")
plt.ylabel("Values")
plt.title("Vertical Boxplot")

"""The annual income is a postively skewed plot with skew=2.15. We have to use the appropriate data transformation techniques like square root, cube root, reciprocal and log to find out which is the best method."""

import numpy as np

sqr_root = np.sqrt(df['Annual_income_x'])
cube_root = np.cbrt(df['Annual_income_x'])
recp = 1/df['Annual_income_x']
log_tran = np.log(df['Annual_income_x'])

plt.figure(figsize=(20,5))
plt.suptitle("Data transformation of annual income column")

plt.subplot(1,4,1)
sns.distplot(sqr_root)
plt.title("Square root transformation")
print("Skew square root:",sqr_root.skew().round(2))

plt.subplot(1,4,2)
sns.distplot(cube_root)
plt.title("Cube root ransformation")
print("Skew cube root:",cube_root.skew().round(2))

plt.subplot(1,4,3)
sns.distplot(recp)
plt.title("Reciprocal transformation")
print("Skew reciprocal:",recp.skew().round(2))

plt.subplot(1,4,4)
sns.distplot(log_tran)
plt.title("Log transformation")
print("Skew log:",log_tran.skew().round(2))

"""The log transformation technique is the best as the skew= 0.06 which is almost a normal distribution curve"""

df['Annual_income_x']=log_tran
df.head()

"""### (b) Label Encoding for categorial variables

This is required to do since the machine learning algorithms only work on the numerical data. That is why there is a need to convert the categorical column into a numerical one.

label encoder
"""

from sklearn.preprocessing import LabelEncoder

columns = ['Gender', 'Car_Owner', 'Property_Owner', 'Type_Income', 'Education', 'Marital_status', 'Housing_type', 'Type_Occupation']

encoder = LabelEncoder()

for column in columns:
    if column in df.columns:
        df[column] = encoder.fit_transform(df[column])
df.head()

"""### (c) Feature Scaling

In Data Processing, we try to change the data in such a way that the model can process it without any problems. And Feature Scaling is one such process in which we transform the data into a better version. Feature Scaling is done to normalize the features in the dataset into a finite range.

Lets perform the train-test split on the dataset. To perform a train-test split, use libraries like scikit-learn in Python. Import the `train_test_split` function, specify the dataset, and set the test size (e.g., 20%). This function randomly divides the data into training and testing sets, preserving the distribution of classes or outcomes.
"""

# Independent variables
X = df.drop(columns=['Ind_ID'])
X

X.info()

# target variable
y = df[['Ind_ID']]
y.head()

X.shape,y.shape

"""**Train Test Split for Classification**

In classification, the data is split into two parts: training and testing sets. The model is trained on a training set, and its performance is examined on a testing set. The training set contains 80% of the data, whereas the test set contains 20%.
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)
X_train.shape,X_test.shape

X_train

X_test

y_train

y_test

"""Lets use Standardization technique by using **StandardScaler()**"""

import pandas as pd

# Print the data types of all columns in X_train
print(X_train.dtypes)

# Identify and convert non-numerical columns
for column in X_train.columns:
    if X_train[column].dtype == 'object':
        try:
            X_train[column] = pd.to_numeric(X_train[column], errors='coerce')
        except:
            # Handle non-convertible values as needed
            pass

X_train

X_test

# both the X_Train and X_test should be converted into dataframe for easy reading
X_train = pd.DataFrame(X_train,columns=X_train.columns)
X_test = pd.DataFrame(X_test,columns=X_test.columns)

X_train

X_test

# to check if the mean = 0 and std = 1 for all columns
X_train.describe().round()

X_test.describe().round()

X_train.info()

"""### (d) Feature Selection

The goal of feature selection techniques in machine learning is to find the best set of features that allows one to build optimized models of studied phenomena.

- Filter Methods: Select features based on statistical metrics like correlation or mutual information.
- Wrapper Methods: Use specific machine learning algorithms to evaluate subsets of features.
- Embedded Methods: Feature selection is inherent to the model training process, such as LASSO regression.

## 5. Machine Learning Model

we will use several machine learning algorithms and find out which is the best ML algorithm.

**In ML, dpending on the problem it is either a classification or a regression.**

We have a binary classification problem.

We shall use the following methods:-
1. Logistic regression
2. Support vector machine (SVM) algorithm
3. Random forest algorithm
4. Decision tree algorithm
5. K-Nearest Neighbour (KNN) algorithm
6. Gradient boost classifier

In a binary classification problem like credit card approval, where Type 1 error (false positives) is considered dangerous, we should prioritize precision over other metrics. A high precision ensures that when the model predicts positive cases (credit card approval), it is likely correct.

For precision, the parameters average = 'binary' and zero_division = 1

So the ML algorithm with the highest precision score will be considered as the best method.
"""

# we will import the accuracy, precision, confusion matrix and classification report for each ML method
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix, f1_score

"""### 1. Logistic Regression"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/home/Final_dataset (1).csv')

# Display the first few rows of the dataframe
print(df.head())

# Load the dataset
df = pd.read_csv('/home/Credit_card (2).csv')

# Display the first few rows of the dataframe
print(df.head())

"""Preprocess the Data
Assuming the dataset has columns Gender, Car_Owner, Property_Owner, Type_Income, Education, Marital_status, Housing_type, Type_Occupation, and Approved (target variable):
"""

import pandas as pd

# Sample data
data = {
    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],
    'Car_Owner': ['Yes', 'No', 'No', 'Yes', 'No'],
    'Property_Owner': ['Yes', 'Yes', 'No', 'No', 'Yes'],
    'Type_Income': ['Salaried', 'Business', 'Salaried', 'Salaried', 'Business'],
    'Education': ['Graduate', 'Undergraduate', 'Graduate', 'PhD', 'Undergraduate'],
    'Marital_status': ['Single', 'Married', 'Single', 'Married', 'Single'],
    'Housing_type': ['Rented', 'Owned', 'Rented', 'Owned', 'Rented'],
    'Type_Occupation': ['Professional', 'Clerical', 'Clerical', 'Professional', 'Professional'],
    'Approved': [1, 0, 1, 1, 0]
}

# Create DataFrame
df = pd.DataFrame(data)

# Save to CSV
df.to_csv('credit_card_data.csv', index=False)

print("Sample credit_card_data.csv created successfully.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Display the first few rows of the dataframe
print(df.head())

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the logistic regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train_scaled, y_train)

# Predict the test data
y_prediction_log = logistic_model.predict(X_test_scaled)

# Calculate performance metrics
accuracy_log = accuracy_score(y_test, y_prediction_log)
precision_score_log = precision_score(y_test, y_prediction_log, average='binary', zero_division=1)
recall_score_log = recall_score(y_test, y_prediction_log)
f1_score_log = f1_score(y_test, y_prediction_log)
confusion_matrix_log = confusion_matrix(y_test, y_prediction_log)
classification_report_log = classification_report(y_test, y_prediction_log)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_log, "\n")
print("Accuracy:", round(accuracy_log * 100, 2), "%\n")
print("Precision:", round(precision_score_log * 100, 2), "%\n")
print("Recall score:", round(recall_score_log * 100, 2), "%\n")
print("F1 score:", round(f1_score_log * 100, 2), "%\n")
print("Classification report:\n", classification_report_log)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_log, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### 2. Support Vector Machines (SVMs)"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the SVM model
svm_model = SVC(kernel='rbf', random_state=42)  # Radial Basis Function (RBF) kernel
svm_model.fit(X_train_scaled, y_train)

# Predict the test data
y_prediction_svm = svm_model.predict(X_test_scaled)

# Calculate performance metrics
accuracy_svm = accuracy_score(y_test, y_prediction_svm)
precision_score_svm = precision_score(y_test, y_prediction_svm, average='binary', zero_division=1)
recall_score_svm = recall_score(y_test, y_prediction_svm)
f1_score_svm = f1_score(y_test, y_prediction_svm)
confusion_matrix_svm = confusion_matrix(y_test, y_prediction_svm)
classification_report_svm = classification_report(y_test, y_prediction_svm)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_svm, "\n")
print("Accuracy:", round(accuracy_svm * 100, 2), "%\n")
print("Precision:", round(precision_score_svm * 100, 2), "%\n")
print("Recall score:", round(recall_score_svm * 100, 2), "%\n")
print("F1 score:", round(f1_score_svm * 100, 2), "%\n")
print("Classification report:\n", classification_report_svm)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_svm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""##3Random Forest"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict the test data
y_prediction_rf = rf_model.predict(X_test)

# Calculate performance metrics
accuracy_rf = accuracy_score(y_test, y_prediction_rf)
precision_score_rf = precision_score(y_test, y_prediction_rf, average='binary', zero_division=1)
recall_score_rf = recall_score(y_test, y_prediction_rf)
f1_score_rf = f1_score(y_test, y_prediction_rf)
confusion_matrix_rf = confusion_matrix(y_test, y_prediction_rf)
classification_report_rf = classification_report(y_test, y_prediction_rf)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_rf, "\n")
print("Accuracy:", round(accuracy_rf * 100, 2), "%\n")
print("Precision:", round(precision_score_rf * 100, 2), "%\n")
print("Recall score:", round(recall_score_rf * 100, 2), "%\n")
print("F1 score:", round(f1_score_rf * 100, 2), "%\n")
print("Classification report:\n", classification_report_rf)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_rf, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""##4 Gradient Boosting##"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train, y_train)

# Predict the test data
y_prediction_gb = gb_model.predict(X_test)

# Calculate performance metrics
accuracy_gb = accuracy_score(y_test, y_prediction_gb)
precision_score_gb = precision_score(y_test, y_prediction_gb, average='binary', zero_division=1)
recall_score_gb = recall_score(y_test, y_prediction_gb)
f1_score_gb = f1_score(y_test, y_prediction_gb)
confusion_matrix_gb = confusion_matrix(y_test, y_prediction_gb)
classification_report_gb = classification_report(y_test, y_prediction_gb)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_gb, "\n")
print("Accuracy:", round(accuracy_gb * 100, 2), "%\n")
print("Precision:", round(precision_score_gb * 100, 2), "%\n")
print("Recall score:", round(recall_score_gb * 100, 2), "%\n")
print("F1 score:", round(f1_score_gb * 100, 2), "%\n")
print("Classification report:\n", classification_report_gb)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_gb, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### 5. Decision Tree"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(X_train, y_train)

# Predict the test data
y_prediction_gb = gb_model.predict(X_test)

# Calculate performance metrics
accuracy_gb = accuracy_score(y_test, y_prediction_gb)
precision_score_gb = precision_score(y_test, y_prediction_gb, average='binary', zero_division=1)
recall_score_gb = recall_score(y_test, y_prediction_gb)
f1_score_gb = f1_score(y_test, y_prediction_gb)
confusion_matrix_gb = confusion_matrix(y_test, y_prediction_gb)
classification_report_gb = classification_report(y_test, y_prediction_gb)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_gb, "\n")
print("Accuracy:", round(accuracy_gb * 100, 2), "%\n")
print("Precision:", round(precision_score_gb * 100, 2), "%\n")
print("Recall score:", round(recall_score_gb * 100, 2), "%\n")
print("F1 score:", round(f1_score_gb * 100, 2), "%\n")
print("Classification report:\n", classification_report_gb)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_gb, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""### 6. Gaussian Naive Bayes"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('credit_card_data.csv')

# Preprocess the data
df = pd.get_dummies(df, drop_first=True)
X = df.drop('Approved', axis=1)
y = df['Approved']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Gaussian Naive Bayes model
gnb_model = GaussianNB()
gnb_model.fit(X_train, y_train)

# Predict the test data
y_prediction_gnb = gnb_model.predict(X_test)

# Calculate performance metrics
accuracy_gnb = accuracy_score(y_test, y_prediction_gnb)
precision_score_gnb = precision_score(y_test, y_prediction_gnb, average='binary', zero_division=1)
recall_score_gnb = recall_score(y_test, y_prediction_gnb)
f1_score_gnb = f1_score(y_test, y_prediction_gnb)
confusion_matrix_gnb = confusion_matrix(y_test, y_prediction_gnb)
classification_report_gnb = classification_report(y_test, y_prediction_gnb)

# Print the performance metrics
print("Confusion matrix:\n", confusion_matrix_gnb, "\n")
print("Accuracy:", round(accuracy_gnb * 100, 2), "%\n")
print("Precision:", round(precision_score_gnb * 100, 2), "%\n")
print("Recall score:", round(recall_score_gnb * 100, 2), "%\n")
print("F1 score:", round(f1_score_gnb * 100, 2), "%\n")
print("Classification report:\n", classification_report_gnb)

# Optional: Plotting the Confusion Matrix
sns.heatmap(confusion_matrix_gnb, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Random Forest:
Ensemble Learning: Random Forest is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees.
Bootstrap Aggregating (Bagging): It uses bagging technique, where each tree in the forest is built from a random sample with replacement (bootstrap sample) of the original dataset.
Feature Randomness: Each tree is trained on a random subset of features, adding an additional layer of randomness. This helps in reducing overfitting and improving the generalization of the model.
Parallel Training: Trees in a Random Forest can be trained in parallel, making it suitable for large datasets.
Less Prone to Overfitting: Due to the randomness introduced in the training process, Random Forests are less prone to overfitting compared to individual decision trees.
Gradient Boosting:
Sequential Ensemble Learning: Gradient Boosting builds trees sequentially, with each tree correcting errors made by the previous one. It optimizes a differentiable loss function, such as mean squared error for regression or log loss for classification.
Gradient Descent Optimization: It minimizes the loss function by gradient descent. In each iteration, it fits a new tree to the residual errors (gradient) of the previous predictions.
Weak Learners: Gradient Boosting typically uses shallow decision trees as weak learners, often referred to as "stumps" (trees with a single split).
Less Parallelism: Unlike Random Forest, Gradient Boosting cannot be parallelized easily because each tree depends on the results of the previous one.
Better Performance: Gradient Boosting often achieves higher predictive performance compared to Random Forests, especially when tuned properly.
Conclusion:
Random Forest is a robust and versatile algorithm that generally works well out of the box with minimal tuning. It's suitable for a wide range of applications, particularly when you need a quick and effective solution without much fine-tuning.
Gradient Boosting tends to offer better predictive performance but requires more careful tuning of hyperparameters. It's particularly useful when you have ample computational resources and are aiming for the highest accuracy possible.
"""